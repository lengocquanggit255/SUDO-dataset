{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13720016,"sourceType":"datasetVersion","datasetId":8729012},{"sourceId":13720078,"sourceType":"datasetVersion","datasetId":8729056}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n# from transformers import BertModel\n# from transformers.modeling_outputs import SequenceClassifierOutput\n# from transformers import PretrainedConfig, PreTrainedModel, BertTokenizer\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport json\nimport numpy as np\nimport fasttext\nfrom sklearn.svm import SVC\nimport pickle\n\nimport warnings\n\n# Bỏ qua tất cả cảnh báo UserWarning (trong đó có InconsistentVersionWarning cũ)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n\nimport joblib\nimport os\n\nnltk.download('punkt')\nnltk.download('punkt_tab')\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n!gunzip cc.en.300.bin.gz   # extracts cc.en.300.bin\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load FastText model\nprint(\"Loading fastText model...\")\nfasttext_model = fasttext.load_model(\"cc.en.300.bin\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:36:58.520476Z","iopub.execute_input":"2025-11-13T14:36:58.521538Z","iopub.status.idle":"2025-11-13T14:38:38.984303Z","shell.execute_reply.started":"2025-11-13T14:36:58.521494Z","shell.execute_reply":"2025-11-13T14:38:38.983058Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"--2025-11-13 14:37:05--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.225.143.54, 13.225.143.109, 13.225.143.122, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.225.143.54|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: ‘cc.en.300.bin.gz’\n\ncc.en.300.bin.gz    100%[===================>]   4.19G   238MB/s    in 15s     \n\n2025-11-13 14:37:20 (284 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n\nLoading fastText model...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def get_embedding(text):\n    return fasttext_model.get_sentence_vector(str(text))\n\nclass SentenceAspectSVM:\n    def __init__(self, aspect, model_path=None):\n        self.aspect = aspect\n        if model_path is None:\n            model_path = f\"/kaggle/input/sudo-svm-phase1/{aspect}_best_model.pkl\"\n        try:\n            with open(model_path, \"rb\") as f:\n                self.model = pickle.load(f)\n            # print(f\"Loaded sentence SVM for {aspect} from {model_path}\")\n        except FileNotFoundError:\n            print(f\"Sentence SVM model not found: {model_path}\")\n            self.model = None\n\n    def filter_positive_sentences(self, sentences):\n        # Trả về các câu dự đoán nhãn 1 (thuộc khía cạnh)\n        if not sentences:\n            return []\n        if self.model is None:\n            # Fallback: không có model thì giữ nguyên để không phá luồng xử lý\n            return sentences\n        try:\n            X = np.vstack([get_embedding(s) for s in sentences])\n            preds = self.model.predict(X)\n            return [s for s, p in zip(sentences, preds) if int(p) == 1]\n        except Exception as e:\n            print(f\"Sentence SVM prediction failed: {e}\")\n            return sentences\n\n\nclass SVMComparativeModel:\n    def __init__(self, aspect, model_path=None):\n        self.aspect = aspect\n        \n        # Load the trained SVM model from pickle file\n        if model_path is None:\n            model_path = f\"/kaggle/input/sudo-svm-checkpoint-phase2/{aspect}_best_model.pkl\"\n        try:\n            with open(model_path, 'rb') as f:\n                self.model = pickle.load(f)\n        except FileNotFoundError:\n            print(f\"Model file not found: {model_path}\")\n            # Create a default SVM model if file doesn't exist\n            self.model = SVC(class_weight=\"balanced\", random_state=42)\n    \n    def predict(self, text_1, text_2):\n        \"\"\"Predict comparison between two texts using FastText embeddings\"\"\"\n        try:\n            # Get FastText embeddings for both texts\n            v1 = get_embedding(text_1)\n            v2 = get_embedding(text_2)\n            \n            # Concatenate embeddings like in training\n            feature_vector = np.concatenate([v1, v2]).reshape(1, -1)\n            \n            # Get prediction\n            pred = self.model.predict(feature_vector)[0]\n            return int(pred)\n        except Exception as e:\n            print(f\"SVM prediction failed: {e}\")\n            return 2  # Fallback to no comparison\n\n\n\nclass OverallModel(nn.Module):\n    def __init__(self, aspect, device):\n        super().__init__()\n        self.aspect = aspect\n        self.device = device\n        \n        self.aspect_index = {'appearance': 0, 'aroma': 1, 'palate': 2, 'taste': 3}\n        self.aspect_idx = self.aspect_index[aspect]  \n\n        # Thay SLAC-BERT bằng SVM câu theo khía cạnh đã train ở phase1\n        self.sentence_svm = SentenceAspectSVM(aspect)\n\n        # Không còn cần tokenizer BERT\n        # self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        \n        # SVM so sánh (giữ nguyên)\n        self.svm_model = SVMComparativeModel(aspect)\n\n    @staticmethod\n    def _truncate_seq(tokens, max_length):\n        while True:\n            total_length = len(tokens)\n            if total_length <= max_length:\n                break\n            tokens.pop()\n            \n        return tokens\n    \n    @staticmethod\n    def _split_clean_sentences(text):\n        sentences = sent_tokenize(text.lower()) \n        sentences = [re.sub(r'\\W+', ' ', s).strip() for s in sentences if len(word_tokenize(s)) > 1]  \n        return sentences\n    \n    def _get_aspect_sentences(self, review_sentences):\n        if len(review_sentences) == 0:\n            return []\n        # Dùng SVM câu theo khía cạnh thay vì SLAC-BERT\n        aspect_sentences = self.sentence_svm.filter_positive_sentences(review_sentences)\n        return aspect_sentences\n        \n    def forward(self, review_1, review_2):\n        review_1_sent = self._split_clean_sentences(review_1)\n        review_2_sent = self._split_clean_sentences(review_2)\n        \n        if len(review_1_sent) == 0 or len(review_2_sent) == 0:\n            return 2\n        \n        review_1_aspect_sentences = self._get_aspect_sentences(review_1_sent)\n        review_2_aspect_sentences = self._get_aspect_sentences(review_2_sent)\n        \n        if len(review_1_aspect_sentences) == 0 or len(review_2_aspect_sentences) == 0:\n            return 2\n\n        # Combine aspect sentences into single texts\n        review_1_text = \" \".join(review_1_aspect_sentences)\n        review_2_text = \" \".join(review_2_aspect_sentences)\n        \n        # Use SVM model for comparison\n        try:\n            result = self.svm_model.predict(review_1_text, review_2_text)\n            return result\n        except Exception as e:\n            print(f\"SVM prediction failed: {e}\")\n            return 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:38:38.986803Z","iopub.execute_input":"2025-11-13T14:38:38.987532Z","iopub.status.idle":"2025-11-13T14:38:39.201412Z","shell.execute_reply.started":"2025-11-13T14:38:38.987496Z","shell.execute_reply":"2025-11-13T14:38:39.199523Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Define labels\nlabels = [-1, 0, 1, 2]\npositive_labels = [-1, 0, 1]  # Positive labels: A<B, A=B, A>B\nnegative_label = 2            # Negative label: No comparison\n\nfrom collections import defaultdict\ndef compute_metrics(true_labels, pred_labels):\n        # Initialize dictionaries to count\n    tp = defaultdict(int)\n    fp = defaultdict(int)\n    fn = defaultdict(int)\n    \n    # Process each sample\n    for true, pred in zip(true_labels, pred_labels):\n        if true == pred:\n            # True Positive: Only count for positive labels\n            if true in positive_labels:\n                tp[true] += 1\n        else:\n            # True is positive, Prediction is null (missing)\n            if true in positive_labels and pred == negative_label:\n                fn[true] += 1  # Only increase FN\n            \n            # True is null, Prediction is positive (excess)\n            elif true == negative_label and pred in positive_labels:\n                fp[pred] += 1  # Only increase FP\n            \n            # Confusion between positive labels\n            elif true in positive_labels and pred in positive_labels:\n                fn[true] += 1  # Missing the correct label\n                fp[pred] += 1  # Excess of the predicted label\n    \n    return tp, fp, fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:38:39.202308Z","iopub.execute_input":"2025-11-13T14:38:39.202596Z","iopub.status.idle":"2025-11-13T14:38:39.609399Z","shell.execute_reply.started":"2025-11-13T14:38:39.202574Z","shell.execute_reply":"2025-11-13T14:38:39.608437Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Define a function to run evaluation for all aspects\ndef overall_evaluate_all_aspects(eval_dataset):\n    aspects = ['appearance', 'aroma', 'palate', 'taste']\n    \n    # Dictionary to store predictions for each aspect\n    aspect_predictions = {}\n    \n    # Run each aspect model and collect predictions\n    for aspect in aspects:\n        model = OverallModel(\n            aspect, \n            device\n        )\n        \n        predictions = []\n        for sample in tqdm(eval_dataset):\n            pred = model(sample['reviewText_1'], sample['reviewText_2'])\n            predictions.append(pred)\n        \n        # Store predictions for this aspect\n        aspect_predictions[aspect] = predictions\n        del model\n    \n    # Combine all predictions and true labels into flat lists\n    all_predictions_flat = []\n    all_true_labels_flat = []\n    \n    for i in range(len(eval_dataset)):\n        for aspect in aspects:\n            all_predictions_flat.append(aspect_predictions[aspect][i])\n            all_true_labels_flat.append(eval_dataset[i][aspect])\n    \n    # Calculate TP, FP, FN\n    tp, fp, fn = compute_metrics(all_true_labels_flat, all_predictions_flat)\n    \n    # Calculate precision, recall, F1 for each class\n    class_metrics = {}\n    for label in positive_labels:\n        # Skip if no instances for this class\n        if tp[label] + fp[label] == 0:\n            precision = 0\n        else:\n            precision = tp[label] / (tp[label] + fp[label])\n            \n        if tp[label] + fn[label] == 0:\n            recall = 0\n        else:\n            recall = tp[label] / (tp[label] + fn[label])\n            \n        if precision + recall == 0:\n            f1 = 0\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n            \n        class_metrics[label] = {\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'tp': tp[label],\n            'fp': fp[label],\n            'fn': fn[label]\n        }\n    \n    # Calculate macro metrics (average over classes)\n    macro_precision = sum(m['precision'] for m in class_metrics.values()) / len(positive_labels)\n    macro_recall = sum(m['recall'] for m in class_metrics.values()) / len(positive_labels)\n    macro_f1 = sum(m['f1'] for m in class_metrics.values()) / len(positive_labels)\n    \n    # Calculate micro metrics (aggregate TP, FP, FN)\n    total_tp = sum(tp[label] for label in positive_labels)\n    total_fp = sum(fp[label] for label in positive_labels)\n    total_fn = sum(fn[label] for label in positive_labels)\n    \n    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n    \n    # Return the combined results\n    return {\n        'micro_f1': micro_f1,\n        'macro_f1': macro_f1,\n        'micro_precision': micro_precision,\n        'macro_precision': macro_precision,\n        'micro_recall': micro_recall,\n        'macro_recall': macro_recall\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:38:39.610538Z","iopub.execute_input":"2025-11-13T14:38:39.610888Z","iopub.status.idle":"2025-11-13T14:38:39.634538Z","shell.execute_reply.started":"2025-11-13T14:38:39.610864Z","shell.execute_reply":"2025-11-13T14:38:39.633387Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Define a function to evaluate each aspect separately\ndef aspect_wise_evaluate(eval_dataset):\n    aspects = ['appearance', 'aroma', 'palate', 'taste']\n    aspect_results = {}\n    \n    for aspect in aspects:\n        print(f\"Evaluating {aspect} model for aspect-wise metrics...\")\n        model = OverallModel(\n            aspect, \n            device\n        )\n        \n        predictions = []\n        true_labels = []\n        \n        for i, sample in enumerate(tqdm(eval_dataset)):\n            pred = model(sample['reviewText_1'], sample['reviewText_2'])\n            predictions.append(pred)\n            true_labels.append(sample[aspect])\n        \n        # Calculate TP, FP, FN for this aspect\n        tp, fp, fn = compute_metrics(true_labels, predictions)\n        \n        # Calculate precision, recall, F1 for each class\n        class_metrics = {}\n        for label in positive_labels:\n            # Skip if no instances for this class\n            if tp[label] + fp[label] == 0:\n                precision = 0\n            else:\n                precision = tp[label] / (tp[label] + fp[label])\n                \n            if tp[label] + fn[label] == 0:\n                recall = 0\n            else:\n                recall = tp[label] / (tp[label] + fn[label])\n                \n            if precision + recall == 0:\n                f1 = 0\n            else:\n                f1 = 2 * precision * recall / (precision + recall)\n                \n            class_metrics[label] = {\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'tp': tp[label],\n                'fp': fp[label],\n                'fn': fn[label]\n            }\n        \n        # Calculate macro metrics (average over classes)\n        macro_precision = sum(m['precision'] for m in class_metrics.values()) / len(positive_labels)\n        macro_recall = sum(m['recall'] for m in class_metrics.values()) / len(positive_labels)\n        macro_f1 = sum(m['f1'] for m in class_metrics.values()) / len(positive_labels)\n        \n        # Calculate micro metrics (aggregate TP, FP, FN)\n        total_tp = sum(tp[label] for label in positive_labels)\n        total_fp = sum(fp[label] for label in positive_labels)\n        total_fn = sum(fn[label] for label in positive_labels)\n        \n        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n        \n        aspect_results[aspect] = {\n            'micro_precision': micro_precision,\n            'micro_recall': micro_recall,\n            'micro_f1': micro_f1,\n            'macro_precision': macro_precision,\n            'macro_recall': macro_recall,\n            'macro_f1': macro_f1,\n            'class_metrics': class_metrics\n        }\n        \n        print(f\"{aspect} - Micro P/R/F1: {micro_precision:.4f}/{micro_recall:.4f}/{micro_f1:.4f}\")\n        print(f\"{aspect} - Macro P/R/F1: {macro_precision:.4f}/{macro_recall:.4f}/{macro_f1:.4f}\")\n        \n        del model\n    \n    return aspect_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:38:39.636713Z","iopub.execute_input":"2025-11-13T14:38:39.637001Z","iopub.status.idle":"2025-11-13T14:38:39.661679Z","shell.execute_reply.started":"2025-11-13T14:38:39.636981Z","shell.execute_reply":"2025-11-13T14:38:39.660563Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import datasets\n\neval_dataset = datasets.load_dataset(\"lengocquangLAB/beer-com-reviews\", split=\"test\")\nresults = overall_evaluate_all_aspects(eval_dataset)\nprint(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:38:39.662732Z","iopub.execute_input":"2025-11-13T14:38:39.663530Z","iopub.status.idle":"2025-11-13T14:39:29.153401Z","shell.execute_reply.started":"2025-11-13T14:38:39.663491Z","shell.execute_reply":"2025-11-13T14:39:29.152129Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591debabc10d4115b2efe2c0eb27b4bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/977k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7368d9f1b284695bcea672c1b35e15a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/142k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92423683e9384212bd30a5b1d7f36ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/133k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3676883e6fe04bc1a5389ade4b488358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b99afaecf14d4189e5120d73df4857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/410 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce411422e6e4133adb016f49ef7dcb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/420 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b21d7ac889740d587ebb5690d62c1d0"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 410/410 [00:06<00:00, 60.51it/s]\n100%|██████████| 410/410 [00:08<00:00, 46.08it/s]\n100%|██████████| 410/410 [00:12<00:00, 32.69it/s]\n100%|██████████| 410/410 [00:14<00:00, 27.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'micro_f1': 0.508029197080292, 'macro_f1': 0.5105251995880314, 'micro_precision': 0.525679758308157, 'macro_precision': 0.5282806575736697, 'micro_recall': 0.4915254237288136, 'macro_recall': 0.49764419759924433}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Run aspect-wise evaluation\nprint(\"=\"*70)\nprint(\"ASPECT-WISE EVALUATION\")\nprint(\"=\"*70)\n\naspect_results = aspect_wise_evaluate(eval_dataset)\n\n# Print summary for each aspect\nfor aspect, metrics in aspect_results.items():\n    print(f\"\\n{aspect.upper()} Results:\")\n    print(f\"  Micro F1: {metrics['micro_f1']:.4f}\")\n    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\nprint()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T14:39:29.154359Z","iopub.execute_input":"2025-11-13T14:39:29.155069Z","iopub.status.idle":"2025-11-13T14:40:12.266291Z","shell.execute_reply.started":"2025-11-13T14:39:29.155032Z","shell.execute_reply":"2025-11-13T14:40:12.265319Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nASPECT-WISE EVALUATION\n======================================================================\nEvaluating appearance model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:06<00:00, 61.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"appearance - Micro P/R/F1: 0.5887/0.5428/0.5648\nappearance - Macro P/R/F1: 0.6205/0.5420/0.5708\nEvaluating aroma model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:08<00:00, 47.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"aroma - Micro P/R/F1: 0.5268/0.4979/0.5119\naroma - Macro P/R/F1: 0.5277/0.5167/0.5127\nEvaluating palate model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:12<00:00, 32.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"palate - Micro P/R/F1: 0.5732/0.5193/0.5449\npalate - Macro P/R/F1: 0.5735/0.5325/0.5485\nEvaluating taste model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:14<00:00, 27.43it/s]","output_type":"stream"},{"name":"stdout","text":"taste - Micro P/R/F1: 0.4594/0.4373/0.4481\ntaste - Macro P/R/F1: 0.4644/0.4403/0.4470\n\nAPPEARANCE Results:\n  Micro F1: 0.5648\n  Macro F1: 0.5708\n\nAROMA Results:\n  Micro F1: 0.5119\n  Macro F1: 0.5127\n\nPALATE Results:\n  Micro F1: 0.5449\n  Macro F1: 0.5485\n\nTASTE Results:\n  Micro F1: 0.4481\n  Macro F1: 0.4470\n\n\nAPPEARANCE Results:\n  Micro F1: 0.5648\n  Macro F1: 0.5708\n\nAROMA Results:\n  Micro F1: 0.5119\n  Macro F1: 0.5127\n\nPALATE Results:\n  Micro F1: 0.5449\n  Macro F1: 0.5485\n\nTASTE Results:\n  Micro F1: 0.4481\n  Macro F1: 0.4470\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7}]}