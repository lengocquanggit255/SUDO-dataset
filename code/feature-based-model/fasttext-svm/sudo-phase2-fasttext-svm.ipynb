{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# ============================\n# 1. Setup: download FastText\n# ============================\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n!gunzip cc.en.300.bin.gz   # extracts cc.en.300.bin","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T07:45:09.995458Z","iopub.execute_input":"2025-10-06T07:45:09.995675Z","iopub.status.idle":"2025-10-06T07:46:19.534249Z","shell.execute_reply.started":"2025-10-06T07:45:09.995649Z","shell.execute_reply":"2025-10-06T07:46:19.533209Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport fasttext\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import GridSearchCV\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport json\nimport pickle\nimport os\n\n# -----------------\n# 0. Setup Phase2 checkpoint folders\n# -----------------\nos.makedirs(\"checkpoints/phase2/best\", exist_ok=True)\n\n# -----------------\n# 1. Load FastText model\n# -----------------\nprint(\"Loading fastText model...\")\nfasttext_model = fasttext.load_model(\"cc.en.300.bin\")\n\ndef get_embedding(text):\n    return fasttext_model.get_sentence_vector(str(text))\n\ndef build_features(df):\n    \"\"\"Concatenate FastText embeddings of sentences_1 & sentences_2\"\"\"\n    X = []\n    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Building features\"):\n        v1 = get_embedding(row[\"text1\"])\n        v2 = get_embedding(row[\"text2\"])\n        vec = np.concatenate([v1, v2])\n        X.append(vec)\n    return np.array(X)\n\n# -----------------\n# 2. Setup aspects & tracking\n# -----------------\naspects = [\"appearance\", \"aroma\", \"palate\", \"taste\"]\nresults = {}\nall_y_test = []\nall_y_test_pred = []\n\n# -----------------\n# 3. SVM hyperparameters\n# -----------------\nparam_grid = {\n    \"kernel\": [\"linear\", \"rbf\"],\n    \"C\": [0.1, 1, 10],\n    \"gamma\": [\"scale\", \"auto\", 0.01, 0.001]\n}\n\n# -----------------\n# 4. Train models for each aspect\n# -----------------\nfor aspect in aspects:\n    print(f\"\\n{'='*50}\")\n    print(f\"Training Phase2 SVM model for aspect: {aspect}\")\n    print(f\"{'='*50}\")\n    \n    # Load roc-language dataset\n    dataset = load_dataset(f\"trungpq/rlcc-new-data-{aspect}\")\n\n    df_train = dataset[\"train\"].to_pandas()\n    df_val   = dataset[\"validation\"].to_pandas()\n    df_test  = dataset[\"test\"].to_pandas()\n\n    # Normalize column names\n    for df in [df_train, df_val, df_test]:\n        df.rename(columns={\n            \"sentences_1\": \"text1\",\n            \"sentences_2\": \"text2\",\n            aspect: \"label\"\n        }, inplace=True)\n        df.dropna(subset=[\"text1\", \"text2\", \"label\"], inplace=True)\n\n    # FastText embeddings\n    print(f\"Building FastText features for {aspect}...\")\n    X_train = build_features(df_train)\n    X_val   = build_features(df_val)\n    X_test  = build_features(df_test)\n    \n    y_train = df_train[\"label\"].values\n    y_val   = df_val[\"label\"].values\n    y_test  = df_test[\"label\"].values\n\n    # Track for combined metrics\n    all_y_test.extend(y_test)\n\n    # Define base SVM\n    svm_model = SVC(class_weight=\"balanced\", random_state=42)\n\n    # GridSearchCV on train+val\n    print(f\"Performing grid search for {aspect}...\")\n    X_trainval = np.vstack([X_train, X_val])\n    y_trainval = np.concatenate([y_train, y_val])\n\n    grid_search = GridSearchCV(\n        svm_model,\n        param_grid,\n        scoring=\"f1_macro\",\n        cv=5,\n        n_jobs=-1,\n        verbose=2\n    )\n\n    with tqdm(total=1, desc=f\"Grid search {aspect}\") as pbar:\n        grid_search.fit(X_trainval, y_trainval)\n        pbar.update(1)\n\n    print(f\"Best Params for {aspect}:\", grid_search.best_params_)\n    print(f\"Best CV Score for {aspect}:\", grid_search.best_score_)\n\n    # Evaluate on test set\n    best_model = grid_search.best_estimator_\n    y_test_pred = best_model.predict(X_test)\n\n    all_y_test_pred.extend(y_test_pred)\n\n    acc = accuracy_score(y_test, y_test_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average=\"macro\")\n\n    # Save metrics\n    results[aspect] = {\n        \"accuracy\": float(acc),\n        \"precision\": float(precision),\n        \"recall\": float(recall),\n        \"f1_score\": float(f1),\n        \"best_params\": grid_search.best_params_,\n        \"best_cv_score\": float(grid_search.best_score_),\n    }\n    \n    # Print evaluation\n    print(f\"\\nFinal Test Evaluation for {aspect}:\")\n    print(f\"Accuracy:  {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1-score:  {f1:.4f}\")\n    print(classification_report(y_test, y_test_pred, digits=4))\n\n    # -----------------\n    # Save PHASE 2 SVM checkpoint\n    # -----------------\n    save_path = f\"checkpoints/phase2/best/{aspect}_best_model.pkl\"\n    with open(save_path, \"wb\") as f:\n        pickle.dump(best_model, f)\n    print(f\"Saved {aspect} model to: {save_path}\")\n\n# -----------------\n# 4.5 Calculate global metrics\n# -----------------\nprecision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n    all_y_test, all_y_test_pred, average=\"macro\"\n)\nprecision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n    all_y_test, all_y_test_pred, average=\"micro\"\n)\n\nresults[\"combined_metrics\"] = {\n    \"macro_precision\": float(precision_macro),\n    \"macro_recall\": float(recall_macro),\n    \"macro_f1_score\": float(f1_macro),\n    \"micro_precision\": float(precision_micro),\n    \"micro_recall\": float(recall_micro),\n    \"micro_f1_score\": float(f1_micro),\n}\n\n# -----------------\n# 6. Save JSON summary\n# -----------------\nwith open(\"phase2_fasttext_svm_results.json\", \"w\") as f:\n    json.dump(results, f, indent=4)\n\nprint(\"\\nSaved → phase2_fasttext_svm_results.json\")\nprint(\"All Phase2 SVM checkpoints saved → checkpoints/phase2/best/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-06T07:46:19.535518Z","iopub.execute_input":"2025-10-06T07:46:19.536124Z","execution_failed":"2025-10-06T07:46:41.225Z"}},"outputs":[],"execution_count":null}]}