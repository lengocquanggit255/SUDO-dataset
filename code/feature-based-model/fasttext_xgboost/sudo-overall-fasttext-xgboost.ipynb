{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport re\nimport json\nimport fasttext\nimport numpy as np\nimport xgboost as xgb\n\nimport warnings\n\n# Bỏ qua tất cả cảnh báo UserWarning (trong đó có InconsistentVersionWarning cũ)\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n\nimport joblib\nimport os\n\n\nnltk.download('punkt')\nnltk.download('punkt_tab')\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n!gunzip cc.en.300.bin.gz   # extracts cc.en.300.bin\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load FastText model and embedding function (same as SVM file)\nprint(\"Loading fastText model...\")\nfasttext_model = fasttext.load_model(\"cc.en.300.bin\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:13:28.437186Z","iopub.execute_input":"2025-11-13T15:13:28.437428Z","iopub.status.idle":"2025-11-13T15:14:49.528191Z","shell.execute_reply.started":"2025-11-13T15:13:28.437404Z","shell.execute_reply":"2025-11-13T15:14:49.527155Z"}},"outputs":[{"name":"stdout","text":"--2025-11-13 15:13:33--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\nResolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.51, 3.163.189.14, 3.163.189.108, ...\nConnecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.51|:443... connected.\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"HTTP request sent, awaiting response... 200 OK\nLength: 4503593528 (4.2G) [application/octet-stream]\nSaving to: ‘cc.en.300.bin.gz’\n\ncc.en.300.bin.gz    100%[===================>]   4.19G   338MB/s    in 12s     \n\n2025-11-13 15:13:46 (357 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n\nLoading fastText model...\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def get_embedding(text):\n    return fasttext_model.get_sentence_vector(str(text))\n\nclass SentenceAspectXGB:\n    def __init__(self, aspect, model_path=None):\n        self.aspect = aspect\n        if model_path is None:\n            model_path = f\"/kaggle/input/sudo-xgboost-checkpoint-phase1/{aspect}_best_model.json\"\n        self.model = xgb.Booster()\n        try:\n            self.model.load_model(model_path)\n        except Exception as e:\n            print(f\"Sentence XGB model not found or failed to load ({model_path}): {e}\")\n            self.model = None\n\n    def filter_positive_sentences(self, sentences, threshold=0.5):\n        if not sentences:\n            return []\n        if self.model is None:\n            # Fallback: keep all to not break downstream logic\n            return sentences\n        try:\n            X = np.vstack([get_embedding(s) for s in sentences])\n            dmat = xgb.DMatrix(X)\n            probs = self.model.predict(dmat)\n            return [s for s, p in zip(sentences, probs) if float(p) > threshold]\n        except Exception as e:\n            print(f\"Sentence XGB predict failed: {e}\")\n            return sentences\n\nclass XGBoostComparativeModel:\n    def __init__(self, aspect):\n        # Use global fastText embedding; do not load another fastText model\n        self.aspect = aspect\n        self.xgb_model = xgb.Booster()\n        model_path = f\"/kaggle/input/sudo-xgboost-checkpoint-phase2/{aspect}_best_model.json\"\n        self.xgb_model.load_model(model_path)\n        self.label2id = {-1: 0, 0: 1, 1: 2}\n        self.id2label = {v: k for k, v in self.label2id.items()}\n    \n    def predict(self, text_1, text_2):\n        emb_1 = get_embedding(text_1)\n        emb_2 = get_embedding(text_2)\n        features = np.concatenate([emb_1, emb_2]).reshape(1, -1)\n        dtest = xgb.DMatrix(features)\n        pred_id = int(self.xgb_model.predict(dtest)[0])\n        return self.id2label[pred_id]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:05.925279Z","iopub.execute_input":"2025-11-13T15:16:05.925647Z","iopub.status.idle":"2025-11-13T15:16:05.934357Z","shell.execute_reply.started":"2025-11-13T15:16:05.925622Z","shell.execute_reply":"2025-11-13T15:16:05.933483Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class OverallMode(nn.Module):\n    def __init__(self, aspect, device):\n        super().__init__()\n        self.aspect = aspect\n        self.device = device\n        \n        self.aspect_index = {'appearance': 0, 'aroma': 1, 'palate': 2, 'taste': 3}\n        self.aspect_idx = self.aspect_index[aspect]  \n        \n        # Replace SLAC-BERT with sentence-level XGBoost\n        self.sentence_xgb = SentenceAspectXGB(aspect)\n        \n        # Load XGBoost model for comparative classification (unchanged)\n        self.xgb_model = XGBoostComparativeModel(aspect)\n            \n    @staticmethod\n    def _truncate_seq(tokens, max_length):\n        while True:\n            total_length = len(tokens)\n            if total_length <= max_length:\n                break\n            tokens.pop()\n            \n        return tokens\n    \n    @staticmethod\n    def _split_clean_sentences(text):\n        sentences = sent_tokenize(text.lower()) \n        sentences = [re.sub(r'\\W+', ' ', s).strip() for s in sentences if len(word_tokenize(s)) > 1]  \n        return sentences\n    \n    def _get_aspect_sentences(self, review_sentences):\n        if len(review_sentences) == 0:\n            return []\n        # Use sentence-level XGBoost classifier to filter aspect sentences\n        aspect_sentences = self.sentence_xgb.filter_positive_sentences(review_sentences)\n        return aspect_sentences\n        \n    def forward(self, review_1, review_2):\n        review_1_sent = self._split_clean_sentences(review_1)\n        review_2_sent = self._split_clean_sentences(review_2)\n        \n        if len(review_1_sent) == 0 or len(review_2_sent) == 0:\n            return 2\n        \n        review_1_aspect_sentences = self._get_aspect_sentences(review_1_sent)\n        review_2_aspect_sentences = self._get_aspect_sentences(review_2_sent)\n        \n        if len(review_1_aspect_sentences) == 0 or len(review_2_aspect_sentences) == 0:\n            return 2\n\n        # Combine aspect sentences into single texts\n        review_1_text = \" \".join(review_1_aspect_sentences)\n        review_2_text = \" \".join(review_2_aspect_sentences)\n        \n        # Use XGBoost model for comparison\n        try:\n            result = self.xgb_model.predict(review_1_text, review_2_text)\n            return result\n        except Exception as e:\n            print(f\"XGBoost prediction failed: {e}\")\n            return 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:05.950873Z","iopub.execute_input":"2025-11-13T15:16:05.951474Z","iopub.status.idle":"2025-11-13T15:16:05.958904Z","shell.execute_reply.started":"2025-11-13T15:16:05.951445Z","shell.execute_reply":"2025-11-13T15:16:05.958316Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Define labels\nlabels = [-1, 0, 1, 2]\npositive_labels = [-1, 0, 1]  # Positive labels: A<B, A=B, A>B\nnegative_label = 2            # Negative label: No comparison\n\nfrom collections import defaultdict\ndef compute_metrics(true_labels, pred_labels):\n        # Initialize dictionaries to count\n    tp = defaultdict(int)\n    fp = defaultdict(int)\n    fn = defaultdict(int)\n    \n    # Process each sample\n    for true, pred in zip(true_labels, pred_labels):\n        if true == pred:\n            # True Positive: Only count for positive labels\n            if true in positive_labels:\n                tp[true] += 1\n        else:\n            # True is positive, Prediction is null (missing)\n            if true in positive_labels and pred == negative_label:\n                fn[true] += 1  # Only increase FN\n            \n            # True is null, Prediction is positive (excess)\n            elif true == negative_label and pred in positive_labels:\n                fp[pred] += 1  # Only increase FP\n            \n            # Confusion between positive labels\n            elif true in positive_labels and pred in positive_labels:\n                fn[true] += 1  # Missing the correct label\n                fp[pred] += 1  # Excess of the predicted label\n    \n    return tp, fp, fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:05.959827Z","iopub.execute_input":"2025-11-13T15:16:05.960059Z","iopub.status.idle":"2025-11-13T15:16:05.977783Z","shell.execute_reply.started":"2025-11-13T15:16:05.960010Z","shell.execute_reply":"2025-11-13T15:16:05.977090Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Define a function to run evaluation for all aspects\ndef overall_evaluate_all_aspects(eval_dataset):\n    aspects = ['appearance', 'aroma', 'palate', 'taste']\n    \n    # Dictionary to store predictions for each aspect\n    aspect_predictions = {}\n    \n    # Run each aspect model and collect predictions\n    for aspect in aspects:\n        model = OverallMode(\n            aspect, \n            device\n        )\n        \n        predictions = []\n        for sample in tqdm(eval_dataset):\n            pred = model(sample['reviewText_1'], sample['reviewText_2'])\n            predictions.append(pred)\n        \n        # Store predictions for this aspect\n        aspect_predictions[aspect] = predictions\n        del model\n    \n    # Combine all predictions and true labels into flat lists\n    all_predictions_flat = []\n    all_true_labels_flat = []\n    \n    for i in range(len(eval_dataset)):\n        for aspect in aspects:\n            all_predictions_flat.append(aspect_predictions[aspect][i])\n            all_true_labels_flat.append(eval_dataset[i][aspect])\n    \n    # Calculate TP, FP, FN\n    tp, fp, fn = compute_metrics(all_true_labels_flat, all_predictions_flat)\n    \n    # Calculate precision, recall, F1 for each class\n    class_metrics = {}\n    for label in positive_labels:\n        # Skip if no instances for this class\n        if tp[label] + fp[label] == 0:\n            precision = 0\n        else:\n            precision = tp[label] / (tp[label] + fp[label])\n            \n        if tp[label] + fn[label] == 0:\n            recall = 0\n        else:\n            recall = tp[label] / (tp[label] + fn[label])\n            \n        if precision + recall == 0:\n            f1 = 0\n        else:\n            f1 = 2 * precision * recall / (precision + recall)\n            \n        class_metrics[label] = {\n            'precision': precision,\n            'recall': recall,\n            'f1': f1,\n            'tp': tp[label],\n            'fp': fp[label],\n            'fn': fn[label]\n        }\n    \n    # Calculate macro metrics (average over classes)\n    macro_precision = sum(m['precision'] for m in class_metrics.values()) / len(positive_labels)\n    macro_recall = sum(m['recall'] for m in class_metrics.values()) / len(positive_labels)\n    macro_f1 = sum(m['f1'] for m in class_metrics.values()) / len(positive_labels)\n    \n    # Calculate micro metrics (aggregate TP, FP, FN)\n    total_tp = sum(tp[label] for label in positive_labels)\n    total_fp = sum(fp[label] for label in positive_labels)\n    total_fn = sum(fn[label] for label in positive_labels)\n    \n    micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n    micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n    micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n    \n    # Return the combined results\n    return {\n        'micro_f1': micro_f1,\n        'macro_f1': macro_f1,\n        'micro_precision': micro_precision,\n        'macro_precision': macro_precision,\n        'micro_recall': micro_recall,\n        'macro_recall': macro_recall\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:05.978700Z","iopub.execute_input":"2025-11-13T15:16:05.978926Z","iopub.status.idle":"2025-11-13T15:16:05.992735Z","shell.execute_reply.started":"2025-11-13T15:16:05.978910Z","shell.execute_reply":"2025-11-13T15:16:05.992118Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Define a function to evaluate each aspect separately\ndef aspect_wise_evaluate(eval_dataset):\n    aspects = ['appearance', 'aroma', 'palate', 'taste']\n    aspect_results = {}\n    \n    for aspect in aspects:\n        print(f\"Evaluating {aspect} model for aspect-wise metrics...\")\n        model = OverallMode(\n            aspect, \n            device\n        )\n        \n        predictions = []\n        true_labels = []\n        \n        for i, sample in enumerate(tqdm(eval_dataset)):\n            pred = model(sample['reviewText_1'], sample['reviewText_2'])\n            predictions.append(pred)\n            true_labels.append(sample[aspect])\n        \n        # Calculate TP, FP, FN for this aspect\n        tp, fp, fn = compute_metrics(true_labels, predictions)\n        \n        # Calculate precision, recall, F1 for each class\n        class_metrics = {}\n        for label in positive_labels:\n            # Skip if no instances for this class\n            if tp[label] + fp[label] == 0:\n                precision = 0\n            else:\n                precision = tp[label] / (tp[label] + fp[label])\n                \n            if tp[label] + fn[label] == 0:\n                recall = 0\n            else:\n                recall = tp[label] / (tp[label] + fn[label])\n                \n            if precision + recall == 0:\n                f1 = 0\n            else:\n                f1 = 2 * precision * recall / (precision + recall)\n                \n            class_metrics[label] = {\n                'precision': precision,\n                'recall': recall,\n                'f1': f1,\n                'tp': tp[label],\n                'fp': fp[label],\n                'fn': fn[label]\n            }\n        \n        # Calculate macro metrics (average over classes)\n        macro_precision = sum(m['precision'] for m in class_metrics.values()) / len(positive_labels)\n        macro_recall = sum(m['recall'] for m in class_metrics.values()) / len(positive_labels)\n        macro_f1 = sum(m['f1'] for m in class_metrics.values()) / len(positive_labels)\n        \n        # Calculate micro metrics (aggregate TP, FP, FN)\n        total_tp = sum(tp[label] for label in positive_labels)\n        total_fp = sum(fp[label] for label in positive_labels)\n        total_fn = sum(fn[label] for label in positive_labels)\n        \n        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        micro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0\n        \n        aspect_results[aspect] = {\n            'micro_precision': micro_precision,\n            'micro_recall': micro_recall,\n            'micro_f1': micro_f1,\n            'macro_precision': macro_precision,\n            'macro_recall': macro_recall,\n            'macro_f1': macro_f1,\n            'class_metrics': class_metrics\n        }\n        \n        print(f\"{aspect} - Micro P/R/F1: {micro_precision:.4f}/{micro_recall:.4f}/{micro_f1:.4f}\")\n        print(f\"{aspect} - Macro P/R/F1: {macro_precision:.4f}/{macro_recall:.4f}/{macro_f1:.4f}\")\n        \n        del model\n    \n    return aspect_results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:06.043695Z","iopub.execute_input":"2025-11-13T15:16:06.043892Z","iopub.status.idle":"2025-11-13T15:16:06.052680Z","shell.execute_reply.started":"2025-11-13T15:16:06.043877Z","shell.execute_reply":"2025-11-13T15:16:06.052034Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import datasets\n\neval_dataset = datasets.load_dataset(\"lengocquangLAB/beer-com-reviews\", split=\"test\")\nresults = overall_evaluate_all_aspects(eval_dataset)\nprint(results)\n\n\n# Run aspect-wise evaluation\nprint(\"=\"*70)\nprint(\"ASPECT-WISE EVALUATION\")\nprint(\"=\"*70)\n\naspect_results = aspect_wise_evaluate(eval_dataset)\n\n# Print summary for each aspect\nfor aspect, metrics in aspect_results.items():\n    print(f\"\\n{aspect.upper()} Results:\")\n    print(f\"  Micro F1: {metrics['micro_f1']:.4f}\")\n    print(f\"  Macro F1: {metrics['macro_f1']:.4f}\")\nprint()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T15:16:06.053816Z","iopub.execute_input":"2025-11-13T15:16:06.054444Z","iopub.status.idle":"2025-11-13T15:16:21.174761Z","shell.execute_reply.started":"2025-11-13T15:16:06.054419Z","shell.execute_reply":"2025-11-13T15:16:21.174036Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 410/410 [00:01<00:00, 265.74it/s]\n100%|██████████| 410/410 [00:01<00:00, 265.39it/s]\n100%|██████████| 410/410 [00:01<00:00, 266.99it/s]\n100%|██████████| 410/410 [00:01<00:00, 246.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"{'micro_f1': 0.46832814122533745, 'macro_f1': 0.464278736718615, 'micro_precision': 0.5219907407407407, 'macro_precision': 0.5415491783372973, 'micro_recall': 0.4246704331450094, 'macro_recall': 0.41536076167800334}\n======================================================================\nASPECT-WISE EVALUATION\n======================================================================\nEvaluating appearance model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:01<00:00, 260.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"appearance - Micro P/R/F1: 0.5514/0.4981/0.5234\nappearance - Macro P/R/F1: 0.6287/0.4803/0.5152\nEvaluating aroma model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:01<00:00, 261.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"aroma - Micro P/R/F1: 0.5722/0.4346/0.4940\naroma - Macro P/R/F1: 0.5834/0.3991/0.4599\nEvaluating palate model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:01<00:00, 266.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"palate - Micro P/R/F1: 0.6204/0.3702/0.4637\npalate - Macro P/R/F1: 0.6506/0.3629/0.4606\nEvaluating taste model for aspect-wise metrics...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 410/410 [00:01<00:00, 249.86it/s]","output_type":"stream"},{"name":"stdout","text":"taste - Micro P/R/F1: 0.4414/0.3920/0.4153\ntaste - Macro P/R/F1: 0.4458/0.3980/0.4198\n\nAPPEARANCE Results:\n  Micro F1: 0.5234\n  Macro F1: 0.5152\n\nAROMA Results:\n  Micro F1: 0.4940\n  Macro F1: 0.4599\n\nPALATE Results:\n  Micro F1: 0.4637\n  Macro F1: 0.4606\n\nTASTE Results:\n  Micro F1: 0.4153\n  Macro F1: 0.4198\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13}]}