{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# ============================\n# 1. Setup: download FastText\n# ============================\n!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n!gunzip cc.en.300.bin.gz   # extracts cc.en.300.bin","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T02:47:14.884359Z","iopub.execute_input":"2025-09-30T02:47:14.884732Z","iopub.status.idle":"2025-09-30T02:49:44.521112Z","shell.execute_reply.started":"2025-09-30T02:47:14.884705Z","shell.execute_reply":"2025-09-30T02:49:44.517365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import fasttext\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, f1_score\nfrom datasets import load_dataset\nimport itertools\nfrom tqdm import tqdm\nimport json\nimport os\n\n# -------------------\n# 0. Create checkpoint folders\n# -------------------\nos.makedirs(\"checkpoints/phase2/best\", exist_ok=True)\nos.makedirs(\"checkpoints/phase2/final\", exist_ok=True)\n\n# -------------------\n# 1. Load pretrained fastText\n# -------------------\nprint(\"Loading fastText model...\")\nmodel = fasttext.load_model(\"cc.en.300.bin\")\n\ndef get_embedding(text):\n    return model.get_sentence_vector(str(text))\n\n# Map nhÃ£n {-1,0,1} -> {0,1,2}\nlabel2id = {-1: 0, 0: 1, 1: 2}\nid2label = {v: k for k, v in label2id.items()}\n\ndef build_features(split_ds, aspect):\n    X, y = [], []\n    for row in tqdm(split_ds, desc=f\"Building features for {aspect}\"):\n        v1 = get_embedding(row[\"sentences_1\"])\n        v2 = get_embedding(row[\"sentences_2\"])\n        vec = np.concatenate([v1, v2])\n        X.append(vec)\n        y.append(label2id[int(row[aspect])])\n    return np.array(X), np.array(y)\n\n# -------------------\n# 2. Train models for each aspect\n# -------------------\naspects = [\"appearance\", \"aroma\", \"palate\", \"taste\"]\nresults = {}\n\nparam_grid = {\n    \"max_depth\": [3],\n    \"eta\": [0.1],\n    \"subsample\": [0.8],\n    \"colsample_bytree\": [0.8],\n    \"min_child_weight\": [1],\n    \"num_boost_round\": [1000]\n}\n\nfor aspect in aspects:\n    print(f\"\\n{'='*50}\")\n    print(f\"Training model for aspect: {aspect}\")\n    print(f\"{'='*50}\")\n\n    dataset = load_dataset(f\"trungpq/rlcc-new-data-{aspect}\")\n\n    def not_null(example):\n        return (\n            example[\"sentences_1\"] is not None\n            and example[\"sentences_2\"] is not None\n            and example[aspect] is not None\n        )\n\n    dataset = dataset.filter(not_null)\n\n    X_train, y_train = build_features(dataset[\"train\"], aspect)\n    X_valid, y_valid = build_features(dataset[\"validation\"], aspect)\n    X_test, y_test = build_features(dataset[\"test\"], aspect)\n\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n\n    num_classes = len(label2id)\n    best_params = None\n    best_model = None\n    best_f1 = -1\n\n    print(f\"Searching best hyperparameters for {aspect}...\")\n    param_combinations = list(itertools.product(*param_grid.values()))\n\n    for values in tqdm(param_combinations, desc=f\"Hyperparameter search - {aspect}\"):\n        params = dict(zip(param_grid.keys(), values))\n        num_boost_round = params.pop(\"num_boost_round\")\n\n        params.update({\n            \"objective\": \"multi:softmax\",\n            \"eval_metric\": \"mlogloss\",\n            \"num_class\": num_classes,\n            \"reg_lambda\": 1.0,\n            \"reg_alpha\": 0.1\n        })\n\n        bst = xgb.train(\n            params=params,\n            dtrain=dtrain,\n            num_boost_round=num_boost_round,\n            evals=[(dvalid, \"valid\")],\n            early_stopping_rounds=500,\n            verbose_eval=False\n        )\n\n        y_pred_val = bst.predict(dvalid)\n        f1 = f1_score(y_valid, y_pred_val, average=\"macro\")\n\n        if f1 > best_f1:\n            best_f1 = f1\n            best_params = params\n            best_model = bst\n\n            print(f\"New best F1={f1:.4f} for {aspect}\")\n\n            # ðŸ”¥ Save best model checkpoint inside phase2 folder\n            best_model.save_model(f\"checkpoints/phase2/best/{aspect}_best_model.json\")\n\n    print(f\"\\nBest Params for {aspect}:\", best_params)\n    print(f\"Best Validation F1 for {aspect}:\", best_f1)\n\n    # -------------------\n    # Retrain with train+validation\n    # -------------------\n    print(f\"\\nRetraining {aspect} model with best params on train+validation...\")\n    X_train_full = np.vstack([X_train, X_valid])\n    y_train_full = np.concatenate([y_train, y_valid])\n\n    dtrain_full = xgb.DMatrix(X_train_full, label=y_train_full)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n\n    final_model = xgb.train(\n        params=best_params,\n        dtrain=dtrain_full,\n        num_boost_round=2000,\n        evals=[(dtest, \"test\")],\n        early_stopping_rounds=250,\n        verbose_eval=False\n    )\n\n    # ðŸ”¥ Save final retrain model\n    final_model.save_model(f\"checkpoints/phase2/final/{aspect}_final_model.json\")\n\n    # -------------------\n    # Final eval\n    # -------------------\n    y_pred_ids = final_model.predict(dtest)\n    y_pred = np.array([id2label[int(i)] for i in y_pred_ids])\n    y_test_true = np.array([id2label[int(i)] for i in y_test])\n\n    acc = accuracy_score(y_test_true, y_pred)\n    precision, recall, f1, _ = precision_recall_fscore_support(y_test_true, y_pred, average=\"macro\")\n\n    results[aspect] = {\n        \"accuracy\": acc,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1_score\": f1,\n        \"best_params\": best_params\n    }\n\n    print(f\"\\nFinal Test Evaluation for {aspect}:\")\n    print(f\"Accuracy:  {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1-score:  {f1:.4f}\")\n    print(classification_report(y_test_true, y_pred, digits=4))\n\n# -------------------\n# Save hyperparams\n# -------------------\nwith open(\"phase2_best_hyperparams_xgboost_fasttext.json\", \"w\") as f:\n    json.dump(results, f, indent=4)\n\nprint(\"\\nAll Phase 2 checkpoint saved into:\")\nprint(\"  checkpoints/phase2/best/\")\nprint(\"  checkpoints/phase2/final/\")\nprint(\"Hyperparameters saved â†’ phase2_best_hyperparams_xgboost_fasttext.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-30T02:49:44.526438Z","iopub.execute_input":"2025-09-30T02:49:44.527358Z","iopub.status.idle":"2025-09-30T02:55:02.889493Z","shell.execute_reply.started":"2025-09-30T02:49:44.527279Z","shell.execute_reply":"2025-09-30T02:55:02.888329Z"}},"outputs":[],"execution_count":null}]}